[project]
name = "gremory"
version = "0.1.0"
description = "Lightweight LLM Inference Server"
readme = "README.md"
requires-python = ">=3.11,<3.12"
dependencies = [
    "litserve>=0.2.3",
    #"llama-cpp-python==0.3.1",
    "llama-cpp-python-cuda==0.3.1; platform_system != 'Darwin'",
    "torch==2.4.1+cu121",
    "transformers>=4.44.2",
    "uvloop>=0.20.0; platform_system != 'Windows'",
    "winloop>=0.1.6; platform_system == 'Windows'",
    "python-dotenv>=1.0.1",
    "flask-cloudflared>=0.0.14",
    "pydantic-settings>=2.5.2",
    "openai>=1.47.0",
]

[tool.uv]
package = true
index-strategy = "unsafe-best-match"
extra-index-url = [
    "https://download.pytorch.org/whl/cu121",
]

[tool.uv.sources]
# llama-cpp-python = [
#     {url = "https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.1/llama_cpp_python-0.3.1-cp311-cp311-win_amd64.whl", marker = "platform_system == 'Windows'"},
#     {url = "https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.1/llama_cpp_python-0.3.1-cp311-cp311-linux_x86_64.whl", marker = "platform_system == 'Linux'"},
#     {url = "https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.1-metal/llama_cpp_python-0.3.1-cp311-cp311-macosx_11_0_arm64.whl", marker = "platform_system == 'Darwin'"}
# ]
llama-cpp-python-cuda = [
    {url = "https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda-0.3.1+cu121-cp311-cp311-win_amd64.whl", marker = "platform_system == 'Windows'"},
    {url = "https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda-0.3.1+cu121-cp311-cp311-linux_x86_64.whl", marker = "platform_system != 'Windows'"}
]